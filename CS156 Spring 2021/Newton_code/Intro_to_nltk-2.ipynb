{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is an introduction to nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takes a while, do not have to run every time\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quote from https://en.wikipedia.org/wiki/Artificial_neural_network\n",
    "example_text = \"An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. The 'signal' at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tokenize\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain.', 'Each connection, like the synapses in a biological brain, can transmit a signal to other neurons.', 'An artificial neuron that receives a signal then processes it and can signal neurons connected to it.', \"The 'signal' at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs.\", 'The connections are called edges.', 'Neurons and edges typically have a weight that adjusts as learning proceeds.', 'The weight increases or decreases the strength of the signal at a connection.', 'Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold.', 'Typically, neurons are aggregated into layers.', 'Different layers may perform different transformations on their inputs.', 'Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['An', 'ANN', 'is', 'based', 'on', 'a', 'collection', 'of', 'connected', 'units', 'or', 'nodes', 'called', 'artificial', 'neurons', ',', 'which', 'loosely', 'model', 'the', 'neurons', 'in', 'a', 'biological', 'brain', '.', 'Each', 'connection', ',', 'like', 'the', 'synapses', 'in', 'a', 'biological', 'brain', ',', 'can', 'transmit', 'a', 'signal', 'to', 'other', 'neurons', '.', 'An', 'artificial', 'neuron', 'that', 'receives', 'a', 'signal', 'then', 'processes', 'it', 'and', 'can', 'signal', 'neurons', 'connected', 'to', 'it', '.', 'The', \"'signal\", \"'\", 'at', 'a', 'connection', 'is', 'a', 'real', 'number', ',', 'and', 'the', 'output', 'of', 'each', 'neuron', 'is', 'computed', 'by', 'some', 'non-linear', 'function', 'of', 'the', 'sum', 'of', 'its', 'inputs', '.', 'The', 'connections', 'are', 'called', 'edges', '.', 'Neurons', 'and', 'edges', 'typically', 'have', 'a', 'weight', 'that', 'adjusts', 'as', 'learning', 'proceeds', '.', 'The', 'weight', 'increases', 'or', 'decreases', 'the', 'strength', 'of', 'the', 'signal', 'at', 'a', 'connection', '.', 'Neurons', 'may', 'have', 'a', 'threshold', 'such', 'that', 'a', 'signal', 'is', 'sent', 'only', 'if', 'the', 'aggregate', 'signal', 'crosses', 'that', 'threshold', '.', 'Typically', ',', 'neurons', 'are', 'aggregated', 'into', 'layers', '.', 'Different', 'layers', 'may', 'perform', 'different', 'transformations', 'on', 'their', 'inputs', '.', 'Signals', 'travel', 'from', 'the', 'first', 'layer', '(', 'the', 'input', 'layer', ')', ',', 'to', 'the', 'last', 'layer', '(', 'the', 'output', 'layer', ')', ',', 'possibly', 'after', 'traversing', 'the', 'layers', 'multiple', 'times', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain.\n",
      "\n",
      "Each connection, like the synapses in a biological brain, can transmit a signal to other neurons.\n",
      "\n",
      "An artificial neuron that receives a signal then processes it and can signal neurons connected to it.\n",
      "\n",
      "The 'signal' at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs.\n",
      "\n",
      "The connections are called edges.\n",
      "\n",
      "Neurons and edges typically have a weight that adjusts as learning proceeds.\n",
      "\n",
      "The weight increases or decreases the strength of the signal at a connection.\n",
      "\n",
      "Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold.\n",
      "\n",
      "Typically, neurons are aggregated into layers.\n",
      "\n",
      "Different layers may perform different transformations on their inputs.\n",
      "\n",
      "Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print each sentence at a time\n",
    "sentences = sent_tokenize(example_text)\n",
    "for s in sentences:\n",
    "    print(s)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['An', 'ANN', 'is', 'based', 'on', 'a', 'collection', 'of', 'connected', 'units', 'or', 'nodes', 'called', 'artificial', 'neurons', ',', 'which', 'loosely', 'model', 'the', 'neurons', 'in', 'a', 'biological', 'brain', '.']\n",
      "\n",
      "['Each', 'connection', ',', 'like', 'the', 'synapses', 'in', 'a', 'biological', 'brain', ',', 'can', 'transmit', 'a', 'signal', 'to', 'other', 'neurons', '.']\n",
      "\n",
      "['An', 'artificial', 'neuron', 'that', 'receives', 'a', 'signal', 'then', 'processes', 'it', 'and', 'can', 'signal', 'neurons', 'connected', 'to', 'it', '.']\n",
      "\n",
      "['The', \"'signal\", \"'\", 'at', 'a', 'connection', 'is', 'a', 'real', 'number', ',', 'and', 'the', 'output', 'of', 'each', 'neuron', 'is', 'computed', 'by', 'some', 'non-linear', 'function', 'of', 'the', 'sum', 'of', 'its', 'inputs', '.']\n",
      "\n",
      "['The', 'connections', 'are', 'called', 'edges', '.']\n",
      "\n",
      "['Neurons', 'and', 'edges', 'typically', 'have', 'a', 'weight', 'that', 'adjusts', 'as', 'learning', 'proceeds', '.']\n",
      "\n",
      "['The', 'weight', 'increases', 'or', 'decreases', 'the', 'strength', 'of', 'the', 'signal', 'at', 'a', 'connection', '.']\n",
      "\n",
      "['Neurons', 'may', 'have', 'a', 'threshold', 'such', 'that', 'a', 'signal', 'is', 'sent', 'only', 'if', 'the', 'aggregate', 'signal', 'crosses', 'that', 'threshold', '.']\n",
      "\n",
      "['Typically', ',', 'neurons', 'are', 'aggregated', 'into', 'layers', '.']\n",
      "\n",
      "['Different', 'layers', 'may', 'perform', 'different', 'transformations', 'on', 'their', 'inputs', '.']\n",
      "\n",
      "['Signals', 'travel', 'from', 'the', 'first', 'layer', '(', 'the', 'input', 'layer', ')', ',', 'to', 'the', 'last', 'layer', '(', 'the', 'output', 'layer', ')', ',', 'possibly', 'after', 'traversing', 'the', 'layers', 'multiple', 'times', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print the list of words in each sentence\n",
    "for s in sentences:\n",
    "    print(word_tokenize(s))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\t\t\tStemmed:\t\t\tLemmatized:\n",
      "An \t\t\t\t An \t\t\t\t An\n",
      "ANN \t\t\t\t ann \t\t\t\t ANN\n",
      "is \t\t\t\t is \t\t\t\t be\n",
      "based \t\t\t\t base \t\t\t\t base\n",
      "on \t\t\t\t on \t\t\t\t on\n",
      "a \t\t\t\t a \t\t\t\t a\n",
      "collection \t\t\t\t collect \t\t\t\t collection\n",
      "of \t\t\t\t of \t\t\t\t of\n",
      "connected \t\t\t\t connect \t\t\t\t connect\n",
      "units \t\t\t\t unit \t\t\t\t units\n",
      "or \t\t\t\t or \t\t\t\t or\n",
      "nodes \t\t\t\t node \t\t\t\t nod\n",
      "called \t\t\t\t call \t\t\t\t call\n",
      "artificial \t\t\t\t artifici \t\t\t\t artificial\n",
      "neurons \t\t\t\t neuron \t\t\t\t neurons\n",
      ", \t\t\t\t , \t\t\t\t ,\n",
      "which \t\t\t\t which \t\t\t\t which\n",
      "loosely \t\t\t\t loos \t\t\t\t loosely\n",
      "model \t\t\t\t model \t\t\t\t model\n",
      "the \t\t\t\t the \t\t\t\t the\n",
      "neurons \t\t\t\t neuron \t\t\t\t neurons\n",
      "in \t\t\t\t in \t\t\t\t in\n",
      "a \t\t\t\t a \t\t\t\t a\n",
      "biological \t\t\t\t biolog \t\t\t\t biological\n",
      "brain \t\t\t\t brain \t\t\t\t brain\n",
      ". \t\t\t\t . \t\t\t\t .\n",
      "Each \t\t\t\t each \t\t\t\t Each\n",
      "connection \t\t\t\t connect \t\t\t\t connection\n",
      ", \t\t\t\t , \t\t\t\t ,\n",
      "like \t\t\t\t like \t\t\t\t like\n",
      "the \t\t\t\t the \t\t\t\t the\n",
      "synapses \t\t\t\t synaps \t\t\t\t synapses\n",
      "in \t\t\t\t in \t\t\t\t in\n",
      "a \t\t\t\t a \t\t\t\t a\n",
      "biological \t\t\t\t biolog \t\t\t\t biological\n",
      "brain \t\t\t\t brain \t\t\t\t brain\n",
      ", \t\t\t\t , \t\t\t\t ,\n",
      "can \t\t\t\t can \t\t\t\t can\n",
      "transmit \t\t\t\t transmit \t\t\t\t transmit\n",
      "a \t\t\t\t a \t\t\t\t a\n",
      "signal \t\t\t\t signal \t\t\t\t signal\n",
      "to \t\t\t\t to \t\t\t\t to\n",
      "other \t\t\t\t other \t\t\t\t other\n",
      "neurons \t\t\t\t neuron \t\t\t\t neurons\n",
      ". \t\t\t\t . \t\t\t\t .\n",
      "An \t\t\t\t An \t\t\t\t An\n",
      "artificial \t\t\t\t artifici \t\t\t\t artificial\n",
      "neuron \t\t\t\t neuron \t\t\t\t neuron\n",
      "that \t\t\t\t that \t\t\t\t that\n",
      "receives \t\t\t\t receiv \t\t\t\t receive\n",
      "a \t\t\t\t a \t\t\t\t a\n",
      "signal \t\t\t\t signal \t\t\t\t signal\n",
      "then \t\t\t\t then \t\t\t\t then\n",
      "processes \t\t\t\t process \t\t\t\t process\n",
      "it \t\t\t\t it \t\t\t\t it\n",
      "and \t\t\t\t and \t\t\t\t and\n",
      "can \t\t\t\t can \t\t\t\t can\n",
      "signal \t\t\t\t signal \t\t\t\t signal\n",
      "neurons \t\t\t\t neuron \t\t\t\t neurons\n",
      "connected \t\t\t\t connect \t\t\t\t connect\n",
      "to \t\t\t\t to \t\t\t\t to\n",
      "it \t\t\t\t it \t\t\t\t it\n",
      ". \t\t\t\t . \t\t\t\t .\n",
      "The \t\t\t\t the \t\t\t\t The\n",
      "'signal \t\t\t\t 'signal \t\t\t\t 'signal\n",
      "' \t\t\t\t ' \t\t\t\t '\n",
      "at \t\t\t\t at \t\t\t\t at\n",
      "a \t\t\t\t a \t\t\t\t a\n",
      "connection \t\t\t\t connect \t\t\t\t connection\n",
      "is \t\t\t\t is \t\t\t\t be\n",
      "a \t\t\t\t a \t\t\t\t a\n",
      "real \t\t\t\t real \t\t\t\t real\n",
      "number \t\t\t\t number \t\t\t\t number\n",
      ", \t\t\t\t , \t\t\t\t ,\n",
      "and \t\t\t\t and \t\t\t\t and\n",
      "the \t\t\t\t the \t\t\t\t the\n",
      "output \t\t\t\t output \t\t\t\t output\n",
      "of \t\t\t\t of \t\t\t\t of\n",
      "each \t\t\t\t each \t\t\t\t each\n",
      "neuron \t\t\t\t neuron \t\t\t\t neuron\n",
      "is \t\t\t\t is \t\t\t\t be\n",
      "computed \t\t\t\t comput \t\t\t\t compute\n",
      "by \t\t\t\t by \t\t\t\t by\n",
      "some \t\t\t\t some \t\t\t\t some\n",
      "non-linear \t\t\t\t non-linear \t\t\t\t non-linear\n",
      "function \t\t\t\t function \t\t\t\t function\n",
      "of \t\t\t\t of \t\t\t\t of\n",
      "the \t\t\t\t the \t\t\t\t the\n",
      "sum \t\t\t\t sum \t\t\t\t sum\n",
      "of \t\t\t\t of \t\t\t\t of\n",
      "its \t\t\t\t it \t\t\t\t its\n",
      "inputs \t\t\t\t input \t\t\t\t input\n",
      ". \t\t\t\t . \t\t\t\t .\n",
      "The \t\t\t\t the \t\t\t\t The\n",
      "connections \t\t\t\t connect \t\t\t\t connections\n",
      "are \t\t\t\t are \t\t\t\t be\n",
      "called \t\t\t\t call \t\t\t\t call\n",
      "edges \t\t\t\t edg \t\t\t\t edge\n",
      ". \t\t\t\t . \t\t\t\t .\n",
      "Neurons \t\t\t\t neuron \t\t\t\t Neurons\n",
      "and \t\t\t\t and \t\t\t\t and\n",
      "edges \t\t\t\t edg \t\t\t\t edge\n",
      "typically \t\t\t\t typic \t\t\t\t typically\n",
      "have \t\t\t\t have \t\t\t\t have\n",
      "a \t\t\t\t a \t\t\t\t a\n",
      "weight \t\t\t\t weight \t\t\t\t weight\n",
      "that \t\t\t\t that \t\t\t\t that\n",
      "adjusts \t\t\t\t adjust \t\t\t\t adjust\n",
      "as \t\t\t\t as \t\t\t\t as\n",
      "learning \t\t\t\t learn \t\t\t\t learn\n",
      "proceeds \t\t\t\t proce \t\t\t\t proceed\n",
      ". \t\t\t\t . \t\t\t\t .\n",
      "The \t\t\t\t the \t\t\t\t The\n",
      "weight \t\t\t\t weight \t\t\t\t weight\n",
      "increases \t\t\t\t increas \t\t\t\t increase\n",
      "or \t\t\t\t or \t\t\t\t or\n",
      "decreases \t\t\t\t decreas \t\t\t\t decrease\n",
      "the \t\t\t\t the \t\t\t\t the\n",
      "strength \t\t\t\t strength \t\t\t\t strength\n",
      "of \t\t\t\t of \t\t\t\t of\n",
      "the \t\t\t\t the \t\t\t\t the\n",
      "signal \t\t\t\t signal \t\t\t\t signal\n",
      "at \t\t\t\t at \t\t\t\t at\n",
      "a \t\t\t\t a \t\t\t\t a\n",
      "connection \t\t\t\t connect \t\t\t\t connection\n",
      ". \t\t\t\t . \t\t\t\t .\n",
      "Neurons \t\t\t\t neuron \t\t\t\t Neurons\n",
      "may \t\t\t\t may \t\t\t\t may\n",
      "have \t\t\t\t have \t\t\t\t have\n",
      "a \t\t\t\t a \t\t\t\t a\n",
      "threshold \t\t\t\t threshold \t\t\t\t threshold\n",
      "such \t\t\t\t such \t\t\t\t such\n",
      "that \t\t\t\t that \t\t\t\t that\n",
      "a \t\t\t\t a \t\t\t\t a\n",
      "signal \t\t\t\t signal \t\t\t\t signal\n",
      "is \t\t\t\t is \t\t\t\t be\n",
      "sent \t\t\t\t sent \t\t\t\t send\n",
      "only \t\t\t\t onli \t\t\t\t only\n",
      "if \t\t\t\t if \t\t\t\t if\n",
      "the \t\t\t\t the \t\t\t\t the\n",
      "aggregate \t\t\t\t aggreg \t\t\t\t aggregate\n",
      "signal \t\t\t\t signal \t\t\t\t signal\n",
      "crosses \t\t\t\t cross \t\t\t\t cross\n",
      "that \t\t\t\t that \t\t\t\t that\n",
      "threshold \t\t\t\t threshold \t\t\t\t threshold\n",
      ". \t\t\t\t . \t\t\t\t .\n",
      "Typically \t\t\t\t typic \t\t\t\t Typically\n",
      ", \t\t\t\t , \t\t\t\t ,\n",
      "neurons \t\t\t\t neuron \t\t\t\t neurons\n",
      "are \t\t\t\t are \t\t\t\t be\n",
      "aggregated \t\t\t\t aggreg \t\t\t\t aggregate\n",
      "into \t\t\t\t into \t\t\t\t into\n",
      "layers \t\t\t\t layer \t\t\t\t layer\n",
      ". \t\t\t\t . \t\t\t\t .\n",
      "Different \t\t\t\t differ \t\t\t\t Different\n",
      "layers \t\t\t\t layer \t\t\t\t layer\n",
      "may \t\t\t\t may \t\t\t\t may\n",
      "perform \t\t\t\t perform \t\t\t\t perform\n",
      "different \t\t\t\t differ \t\t\t\t different\n",
      "transformations \t\t\t\t transform \t\t\t\t transformations\n",
      "on \t\t\t\t on \t\t\t\t on\n",
      "their \t\t\t\t their \t\t\t\t their\n",
      "inputs \t\t\t\t input \t\t\t\t input\n",
      ". \t\t\t\t . \t\t\t\t .\n",
      "Signals \t\t\t\t signal \t\t\t\t Signals\n",
      "travel \t\t\t\t travel \t\t\t\t travel\n",
      "from \t\t\t\t from \t\t\t\t from\n",
      "the \t\t\t\t the \t\t\t\t the\n",
      "first \t\t\t\t first \t\t\t\t first\n",
      "layer \t\t\t\t layer \t\t\t\t layer\n",
      "( \t\t\t\t ( \t\t\t\t (\n",
      "the \t\t\t\t the \t\t\t\t the\n",
      "input \t\t\t\t input \t\t\t\t input\n",
      "layer \t\t\t\t layer \t\t\t\t layer\n",
      ") \t\t\t\t ) \t\t\t\t )\n",
      ", \t\t\t\t , \t\t\t\t ,\n",
      "to \t\t\t\t to \t\t\t\t to\n",
      "the \t\t\t\t the \t\t\t\t the\n",
      "last \t\t\t\t last \t\t\t\t last\n",
      "layer \t\t\t\t layer \t\t\t\t layer\n",
      "( \t\t\t\t ( \t\t\t\t (\n",
      "the \t\t\t\t the \t\t\t\t the\n",
      "output \t\t\t\t output \t\t\t\t output\n",
      "layer \t\t\t\t layer \t\t\t\t layer\n",
      ") \t\t\t\t ) \t\t\t\t )\n",
      ", \t\t\t\t , \t\t\t\t ,\n",
      "possibly \t\t\t\t possibl \t\t\t\t possibly\n",
      "after \t\t\t\t after \t\t\t\t after\n",
      "traversing \t\t\t\t travers \t\t\t\t traverse\n",
      "the \t\t\t\t the \t\t\t\t the\n",
      "layers \t\t\t\t layer \t\t\t\t layer\n",
      "multiple \t\t\t\t multipl \t\t\t\t multiple\n",
      "times \t\t\t\t time \t\t\t\t time\n",
      ". \t\t\t\t . \t\t\t\t .\n"
     ]
    }
   ],
   "source": [
    "def stem_and_lemmatize(stemmer, lemmatizer, word):\n",
    "    print(word,\"\\t\\t\\t\\t\", stemmer.stem(word),\"\\t\\t\\t\\t\",lemmatizer.lemmatize(word, pos = wordnet.VERB))\n",
    "\n",
    "print(\"Original\\t\\t\\tStemmed:\\t\\t\\tLemmatized:\")\n",
    "for w in word_tokenize(example_text):\n",
    "    stem_and_lemmatize(stemmer, lemmatizer, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words are irrelevant words that are generally filtered out prior to processing corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.g. \"the\", \"and\", \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['An', 'ANN', 'based', 'collection', 'connected', 'units', 'nodes', 'called', 'artificial', 'neurons', ',', 'loosely', 'model', 'neurons', 'biological', 'brain', '.', 'Each', 'connection', ',', 'like', 'synapses', 'biological', 'brain', ',', 'transmit', 'signal', 'neurons', '.', 'An', 'artificial', 'neuron', 'receives', 'signal', 'processes', 'signal', 'neurons', 'connected', '.', 'The', \"'signal\", \"'\", 'connection', 'real', 'number', ',', 'output', 'neuron', 'computed', 'non-linear', 'function', 'sum', 'inputs', '.', 'The', 'connections', 'called', 'edges', '.', 'Neurons', 'edges', 'typically', 'weight', 'adjusts', 'learning', 'proceeds', '.', 'The', 'weight', 'increases', 'decreases', 'strength', 'signal', 'connection', '.', 'Neurons', 'may', 'threshold', 'signal', 'sent', 'aggregate', 'signal', 'crosses', 'threshold', '.', 'Typically', ',', 'neurons', 'aggregated', 'layers', '.', 'Different', 'layers', 'may', 'perform', 'different', 'transformations', 'inputs', '.', 'Signals', 'travel', 'first', 'layer', '(', 'input', 'layer', ')', ',', 'last', 'layer', '(', 'output', 'layer', ')', ',', 'possibly', 'traversing', 'layers', 'multiple', 'times', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "words = word_tokenize(example_text)\n",
    "text_without_stop_words = [w for w in words if not w in stop_words]\n",
    "print(text_without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An ANN is based on a collection of connected units or nodes called artificial neurons  which loosely model the neurons in a biological brain  Each connection  like the synapses in a biological brain  can transmit a signal to other neurons  An artificial neuron that receives a signal then processes it and can signal neurons connected to it  The  signal  at a connection is a real number  and the output of each neuron is computed by some non linear function of the sum of its inputs  The connections are called edges  Neurons and edges typically have a weight that adjusts as learning proceeds  The weight increases or decreases the strength of the signal at a connection  Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold  Typically  neurons are aggregated into layers  Different layers may perform different transformations on their inputs  Signals travel from the first layer  the input layer   to the last layer  the output layer   possibly after traversing the layers multiple times \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "search_pattern = r\"[^\\w]\"\n",
    "print(re.sub(search_pattern, \" \", example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ all non-words (punctuation, contractions, etc.) are taken out of this text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adjusts</th>\n",
       "      <th>after</th>\n",
       "      <th>aggregate</th>\n",
       "      <th>aggregated</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>ann</th>\n",
       "      <th>are</th>\n",
       "      <th>artificial</th>\n",
       "      <th>as</th>\n",
       "      <th>...</th>\n",
       "      <th>times</th>\n",
       "      <th>to</th>\n",
       "      <th>transformations</th>\n",
       "      <th>transmit</th>\n",
       "      <th>travel</th>\n",
       "      <th>traversing</th>\n",
       "      <th>typically</th>\n",
       "      <th>units</th>\n",
       "      <th>weight</th>\n",
       "      <th>which</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    adjusts  after  aggregate  aggregated  an  and  ann  are  artificial  as  \\\n",
       "0         0      0          0           0   1    0    1    0           1   0   \n",
       "1         0      0          0           0   0    0    0    0           0   0   \n",
       "2         0      0          0           0   1    1    0    0           1   0   \n",
       "3         0      0          0           0   0    1    0    0           0   0   \n",
       "4         0      0          0           0   0    0    0    1           0   0   \n",
       "5         1      0          0           0   0    1    0    0           0   1   \n",
       "6         0      0          0           0   0    0    0    0           0   0   \n",
       "7         0      0          1           0   0    0    0    0           0   0   \n",
       "8         0      0          0           1   0    0    0    1           0   0   \n",
       "9         0      0          0           0   0    0    0    0           0   0   \n",
       "10        0      1          0           0   0    0    0    0           0   0   \n",
       "\n",
       "    ...  times  to  transformations  transmit  travel  traversing  typically  \\\n",
       "0   ...      0   0                0         0       0           0          0   \n",
       "1   ...      0   1                0         1       0           0          0   \n",
       "2   ...      0   1                0         0       0           0          0   \n",
       "3   ...      0   0                0         0       0           0          0   \n",
       "4   ...      0   0                0         0       0           0          0   \n",
       "5   ...      0   0                0         0       0           0          1   \n",
       "6   ...      0   0                0         0       0           0          0   \n",
       "7   ...      0   0                0         0       0           0          0   \n",
       "8   ...      0   0                0         0       0           0          1   \n",
       "9   ...      0   0                1         0       0           0          0   \n",
       "10  ...      1   1                0         0       1           1          0   \n",
       "\n",
       "    units  weight  which  \n",
       "0       1       0      1  \n",
       "1       0       0      0  \n",
       "2       0       0      0  \n",
       "3       0       0      0  \n",
       "4       0       0      0  \n",
       "5       0       1      0  \n",
       "6       0       1      0  \n",
       "7       0       0      0  \n",
       "8       0       0      0  \n",
       "9       0       0      0  \n",
       "10      0       0      0  \n",
       "\n",
       "[11 rows x 90 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "#design vocabulary\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "#create vectorized representation\n",
    "#here we are going to create a bag-of-words model for each sentence of the example_text\n",
    "bag_of_words = count_vectorizer.fit_transform(sent_tokenize(example_text))\n",
    "\n",
    "features = count_vectorizer.get_feature_names()\n",
    "pd.DataFrame(bag_of_words.toarray(), columns = features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adjusts</th>\n",
       "      <th>after</th>\n",
       "      <th>aggregate</th>\n",
       "      <th>aggregated</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>ann</th>\n",
       "      <th>are</th>\n",
       "      <th>artificial</th>\n",
       "      <th>as</th>\n",
       "      <th>...</th>\n",
       "      <th>times</th>\n",
       "      <th>to</th>\n",
       "      <th>transformations</th>\n",
       "      <th>transmit</th>\n",
       "      <th>travel</th>\n",
       "      <th>traversing</th>\n",
       "      <th>typically</th>\n",
       "      <th>units</th>\n",
       "      <th>weight</th>\n",
       "      <th>which</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206253</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206253</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227444</td>\n",
       "      <td>0.200024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.345929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.260041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295688</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285384</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.247573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.480558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.410763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.410763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.168557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168557</td>\n",
       "      <td>0.126707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.168557</td>\n",
       "      <td>0.168557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     adjusts     after  aggregate  aggregated        an       and       ann  \\\n",
       "0   0.000000  0.000000   0.000000    0.000000  0.206253  0.000000  0.241299   \n",
       "1   0.000000  0.000000   0.000000    0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.000000  0.000000   0.000000    0.000000  0.227444  0.200024  0.000000   \n",
       "3   0.000000  0.000000   0.000000    0.000000  0.000000  0.151015  0.000000   \n",
       "4   0.000000  0.000000   0.000000    0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.345929  0.000000   0.000000    0.000000  0.000000  0.260041  0.000000   \n",
       "6   0.000000  0.000000   0.000000    0.000000  0.000000  0.000000  0.000000   \n",
       "7   0.000000  0.000000   0.247573    0.000000  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.000000   0.000000    0.480558  0.000000  0.000000  0.000000   \n",
       "9   0.000000  0.000000   0.000000    0.000000  0.000000  0.000000  0.000000   \n",
       "10  0.000000  0.168557   0.000000    0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         are  artificial        as  ...     times        to  transformations  \\\n",
       "0   0.000000    0.206253  0.000000  ...  0.000000  0.000000         0.000000   \n",
       "1   0.000000    0.000000  0.000000  ...  0.000000  0.241257         0.000000   \n",
       "2   0.000000    0.227444  0.000000  ...  0.000000  0.200024         0.000000   \n",
       "3   0.000000    0.000000  0.000000  ...  0.000000  0.000000         0.000000   \n",
       "4   0.460503    0.000000  0.000000  ...  0.000000  0.000000         0.000000   \n",
       "5   0.000000    0.000000  0.345929  ...  0.000000  0.000000         0.000000   \n",
       "6   0.000000    0.000000  0.000000  ...  0.000000  0.000000         0.000000   \n",
       "7   0.000000    0.000000  0.000000  ...  0.000000  0.000000         0.000000   \n",
       "8   0.410763    0.000000  0.000000  ...  0.000000  0.000000         0.000000   \n",
       "9   0.000000    0.000000  0.000000  ...  0.000000  0.000000         0.320142   \n",
       "10  0.000000    0.000000  0.000000  ...  0.168557  0.126707         0.000000   \n",
       "\n",
       "    transmit    travel  traversing  typically     units    weight     which  \n",
       "0   0.000000  0.000000    0.000000   0.000000  0.241299  0.000000  0.241299  \n",
       "1   0.320941  0.000000    0.000000   0.000000  0.000000  0.000000  0.000000  \n",
       "2   0.000000  0.000000    0.000000   0.000000  0.000000  0.000000  0.000000  \n",
       "3   0.000000  0.000000    0.000000   0.000000  0.000000  0.000000  0.000000  \n",
       "4   0.000000  0.000000    0.000000   0.000000  0.000000  0.000000  0.000000  \n",
       "5   0.000000  0.000000    0.000000   0.295688  0.000000  0.295688  0.000000  \n",
       "6   0.000000  0.000000    0.000000   0.000000  0.000000  0.285384  0.000000  \n",
       "7   0.000000  0.000000    0.000000   0.000000  0.000000  0.000000  0.000000  \n",
       "8   0.000000  0.000000    0.000000   0.410763  0.000000  0.000000  0.000000  \n",
       "9   0.000000  0.000000    0.000000   0.000000  0.000000  0.000000  0.000000  \n",
       "10  0.000000  0.168557    0.168557   0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[11 rows x 90 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "#design vocabulary\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "#create vectorized representation\n",
    "#here we are going to create a bag-of-words model for each sentence of the example_text\n",
    "bag_of_words = tfidf_vectorizer.fit_transform(sent_tokenize(example_text))\n",
    "\n",
    "features = tfidf_vectorizer.get_feature_names()\n",
    "pd.DataFrame(bag_of_words.toarray(), columns = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
